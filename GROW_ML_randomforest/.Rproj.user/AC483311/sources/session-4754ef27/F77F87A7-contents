## ---------------------------
##
## Script: GRWO_ML_randomforest_fctreg
##
## Purpose: train random forest regression model to predict individual functions by landuse
##
## Author: Ikaia Leleiwi
##
## Date Created: May 26th 2023
##
## Copyright (c) Ikaia Leleiwi, 2023
## Email: ileleiwi@gmail.com
##
## ---------------------------
##
## Notes:
#       Random Forest Regression R^2 Results
#       Aerobic: 0.472
#       Microaerophilic: 0.469
#       Light_driven: 0.520
#       N_Reducer: -0.405
#       DNRA: 0.479
#       Methanotroph: -0.347
##   
##
## ---------------------------

## set working directory

setwd(paste0("/Users/ikaialeleiwi/Desktop/Lab/GROW_ML"))

## ---------------------------

##Libraries

library(tidyverse)
library(caret)
library(pROC)
library(ranger) #importance_pvalues
set.seed(123)

##Data

target_col <- c("Aerobic", "Microaerophillic", "Light_driven", "N_Reducer",
                "DNRA", "Methanotroph")

df <- read_csv("data/metat_metag.csv") %>%
  select(target_col[6], starts_with("Pct"), DamDensWs, PopDen2010Ws) %>%
  rename("target" = target_col[6]) 

#check for all 0's or sparce (only 1)
df_clean <- df %>%
  select(where(~ any(. !=0 ))) %>% #remove columns with all 0's and all FALSE
  select(where(~ any(. !=1 ))) %>%
  mutate_all(function(x) x + 1e-9) %>% #add pseudocount
  mutate_all(log10) #log transform


#look at data
# plot_hist <- function(var, name){
#   
#   ggplot(df_clean, aes(x = (var))) +
#     geom_histogram(bins = 100) +
#     labs(x = name,
#          title = name)
#   
# }
# 
# hist_list <- df_clean %>%
#   select_if(is.numeric) %>%
#   map2(.y = names(.),
#        ~plot_hist(.x, .y)) 
# 
# map(hist_list, ~print(.x))

#drop fire and ice b/c they have very few observations (this only improved R^2 by 0.04)
df_clean <- df_clean %>%
  select(-PctFire2010Ws, -PctIce2016Ws)

#split data
training_partition <- df_clean$target %>%
  createDataPartition(p=0.75, list = FALSE)

train <- df_clean[training_partition,]
test <- df_clean[-training_partition,]

#resampling method
fit_control <- trainControl(## 10-fold CV
  method = "cv",
  number = 10,
  search = 'grid', 
  savePredictions = TRUE)

#tuning paramaters
rang_grid <- expand.grid(mtry = seq(2,8,2),
                         splitrule = c("variance", "extratrees", "maxstat"),
                         min.node.size = c(1, 3, 5, 6))

rf_grid <- expand.grid(.mtry = c(sqrt(ncol(df_clean))))

#fit a random forest model (using ranger)
rang_fit <- train(target ~ .,
                  data = train,
                  method = "ranger",
                  trControl = fit_control,
                  tuneGrid = rang_grid,
                  importance = 'impurity',
                  metric = "Rsquared")

rang_fit


# predict the outcome on a test set
rang_pred <- predict(rang_fit, test)

plot(test$target ~ rang_pred)

#metrics
d <- test$target - rang_pred
mse = mean((d)^2)
mae = mean(abs(d))
rmse = sqrt(mse)
R2 = 1-(sum((d)^2)/sum((test$target-mean(test$target))^2))
cat(" MAE:", mae, "\n", "MSE:", mse, "\n", 
    "RMSE:", rmse, "\n", "R-squared:", R2)